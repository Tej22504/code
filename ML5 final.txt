#!/usr/bin/env python
# coding: utf-8

# In[1]:


# Step 1: Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage


# In[2]:


# Attempt to read the dataset with a different encoding
df = pd.read_csv('sales1.csv', encoding='ISO-8859-1')


# In[3]:


df.head()


# In[4]:


df.tail()


# In[5]:


df.isnull()


# In[6]:


df.isnull().sum()


# In[7]:


df.notnull()


# In[8]:


df.notnull().sum()


# In[9]:


df.describe()


# In[10]:


df.info()


# In[11]:


# Remove the column 'addressline 2'
df = df.drop(columns=['ADDRESSLINE2'], errors='ignore')


# In[12]:


df.info()


# In[13]:


# Step 4: Preprocess the data
X = df.select_dtypes(include=[np.number])  # Selecting only numeric columns


# In[14]:



# Standardizing the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


# In[15]:


# Step 5: Determine the optimal number of clusters using the elbow method
inertia = []
K = range(1, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)


# In[16]:


# Plotting the elbow curve
plt.figure(figsize=(8, 5))
plt.plot(K, inertia, marker='o')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.xticks(K)
plt.grid()
plt.show()


# In[17]:


# Step 6: Apply K-Means clustering with the optimal number of clusters
optimal_k = 4  # Replace this with the optimal number you find from the elbow method
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(X_scaled)


# In[18]:


# Add the cluster labels to the original DataFrame
df['Cluster'] = clusters


# In[19]:


# Step 7: Apply Hierarchical clustering
linked = linkage(X_scaled, 'ward')

# Step 8: Plotting the dendrogram for Hierarchical clustering
plt.figure(figsize=(10, 7))
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title('Dendrogram for Hierarchical Clustering')
plt.xlabel('Samples')
plt.ylabel('Distance')
plt.show()


# In[20]:


# Optional: Visualizing K-Means Clusters
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters, palette='viridis', alpha=0.7)
plt.title('K-Means Clustering Visualization')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend()
plt.show()


# In[ ]:




