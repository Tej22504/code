#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error


# In[7]:


# Load the dataset
df = pd.read_csv('uber1.csv')


# In[8]:


df.head()


# In[9]:


# Display the last few rows of the dataset
df.tail()


# In[10]:


#Size of dataset
df.shape


# In[11]:


#Columns In dataset
df.info()


# In[12]:


#Description of dataset
df.describe()


# In[13]:


# Check for missing values
print(df.isnull().sum())


# In[16]:


# Convert pickup_datetime to datetime format if it's not already
df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])


# In[17]:


# Extract useful information from datetime
df['hour'] = df['pickup_datetime'].dt.hour
df['day_of_week'] = df['pickup_datetime'].dt.dayofweek


# In[18]:


# Check for missing values
print(df.isnull().sum())


# In[19]:


# Remove null values
df.dropna(inplace=True)


# In[20]:


# Verify that there are no missing values left
print(df.isnull().sum())


# In[21]:


# Visualize outliers using boxplots
plt.figure(figsize=(12, 6))
sns.boxplot(data=df[['fare_amount']])
plt.show()


# In[22]:


# Remove outliers based on fare_amount
Q1 = df['fare_amount'].quantile(0.25)
Q3 = df['fare_amount'].quantile(0.75)
IQR = Q3 - Q1


# In[23]:


# Define lower and upper bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR


# In[24]:


# Filter out outliers
df = df[(df['fare_amount'] >= lower_bound) & (df['fare_amount'] <= upper_bound)]
# Identify outliers
outliers = df[(df['fare_amount'] < lower_bound) | (df['fare_amount'] > upper_bound)]


# In[25]:


# Display the outliers
print("Outliers based on fare_amount:")
print(outliers)


# In[26]:


# Recheck the boxplot after removing outliers
plt.figure(figsize=(12, 6))
sns.boxplot(data=df[['fare_amount']])
plt.show()


# In[27]:


# Select only the numeric columns for the correlation matrix
numeric_df = df.select_dtypes(include=[np.number])

# Calculate the correlation matrix
corr_matrix = numeric_df.corr()

# Display the correlation matrix
print(corr_matrix)


# In[28]:


# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Numeric Features')
plt.show()


# In[29]:


# Select features based on correlation
features = ['hour', 'day_of_week', 'passenger_count']  # Modify based on correlation matrix
target = 'fare_amount'


# In[30]:


# Split the data into train and test sets
X = df[features]
y = df[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# In[31]:


# Initialize and train the Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Predict using the model
y_pred_lr = lr_model.predict(X_test)


# In[32]:


# Initialize and train the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict using the model
y_pred_rf = rf_model.predict(X_test)


# In[33]:


# Define a function to calculate evaluation metrics
def evaluate_model(y_test, y_pred, model_name):
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    print(f"{model_name} - R2 Score: {r2:.2f}")
    print(f"{model_name} - RMSE: {rmse:.2f}")

# Evaluate Linear Regression
evaluate_model(y_test, y_pred_lr, 'Linear Regression')

# Evaluate Random Forest Regression
evaluate_model(y_test, y_pred_rf, 'Random Forest Regression')


# In[ ]:




